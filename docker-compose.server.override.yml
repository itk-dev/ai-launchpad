services:
  llm-proxy:
    image: haproxytech/haproxy-alpine:3.0
    networks:
      - app
    ports:
      - "80"
    depends_on:
      - ollama
    volumes:
      - ./.docker/proxy/:/usr/local/etc/haproxy:ro

  vllm:
    image: vllm/vllm-openai
    command: "--model ${VLLM_MODEL:-facebook/opt-125m} ${VLLM_OPTIONS:-''}"
    shm_size: "${SHM_SIZE:-8gb}"
    networks:
      - app
    ports:
      - "8000"
    environment:
      - "HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}"
    volumes:
      - ./.docker/data/vllm/:/root/.cache/huggingface:rw
    # Add support for GPU on the server.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]

  ollama:
    image: ollama/ollama:0.1.44
    networks:
      - app
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "11434"
    environment:
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-12}
      - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS:-2}
    volumes:
      - ./.docker/data/ollama:/root/.ollama
    # Add support for GPU on the server.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
